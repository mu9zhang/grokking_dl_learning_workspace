{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: [ 0.540848  0.72112  -0.400432]\n",
      "Error: 2.6561231104\n",
      "\n",
      "Weight: [ 0.49944225  0.84194128 -0.28583891]\n",
      "Error: 0.9628701776715985\n",
      "\n",
      "Weight: [ 0.43945363  0.91252555 -0.23598163]\n",
      "Error: 0.5509165866836797\n",
      "\n",
      "Weight: [ 0.38100946  0.95888996 -0.20924067]\n",
      "Error: 0.36445836852222424\n",
      "\n",
      "Weight: [ 0.32955689  0.99146612 -0.19117291]\n",
      "Error: 0.2516768662079895\n",
      "\n",
      "Weight: [ 0.28576471  1.01500784 -0.17673288]\n",
      "Error: 0.17797575048089034\n",
      "\n",
      "Weight: [ 0.24888702  1.03207349 -0.16412797]\n",
      "Error: 0.12864460733422164\n",
      "\n",
      "Weight: [ 0.21787885  1.0442761  -0.15269426]\n",
      "Error: 0.09511036950476208\n",
      "\n",
      "Weight: [ 0.19174429  1.05275286 -0.14216186]\n",
      "Error: 0.07194564247043436\n",
      "\n",
      "Weight: [ 0.16962672  1.05835728 -0.1324006 ]\n",
      "Error: 0.05564914990717743\n",
      "\n",
      "Weight: [ 0.15081598  1.0617496  -0.12333176]\n",
      "Error: 0.04394763937673939\n",
      "\n",
      "Weight: [ 0.13473149  1.06344721 -0.11489715]\n",
      "Error: 0.035357967050948465\n",
      "\n",
      "Weight: [ 0.12090095  1.06385744 -0.10704822]\n",
      "Error: 0.02890700056547436\n",
      "\n",
      "Weight: [ 0.10894061  1.06330118 -0.099742  ]\n",
      "Error: 0.023951660591138853\n",
      "\n",
      "Weight: [ 0.0985384   1.06203086 -0.09293946]\n",
      "Error: 0.020063105176016144\n",
      "\n",
      "Weight: [ 0.08944019  1.06024455 -0.08660484]\n",
      "Error: 0.016952094519447087\n",
      "\n",
      "Weight: [ 0.08143858  1.05809708 -0.08070513]\n",
      "Error: 0.014420818295271236\n",
      "\n",
      "Weight: [ 0.07436386  1.05570893 -0.07520984]\n",
      "Error: 0.012331739998443648\n",
      "\n",
      "Weight: [ 0.06807682  1.05317333 -0.07009074]\n",
      "Error: 0.010587393171639842\n",
      "\n",
      "Weight: [ 0.06246285  1.05056186 -0.06532169]\n",
      "Error: 0.009117233405426495\n",
      "\n",
      "Weight: [ 0.05742729  1.04792901 -0.06087842]\n",
      "Error: 0.00786904226904208\n",
      "\n",
      "Weight: [ 0.05289162  1.04531576 -0.05673843]\n",
      "Error: 0.006803273214640502\n",
      "\n",
      "Weight: [ 0.0487905   1.04275242 -0.0528808 ]\n",
      "Error: 0.005889303541837786\n",
      "\n",
      "Weight: [ 0.04506924  1.04026098 -0.04928611]\n",
      "Error: 0.0051029252561172675\n",
      "\n",
      "Weight: [ 0.04168187  1.03785688 -0.04593632]\n",
      "Error: 0.004424644608684828\n",
      "\n",
      "Weight: [ 0.03858958  1.03555045 -0.04281463]\n",
      "Error: 0.0038385124412518303\n",
      "\n",
      "Weight: [ 0.03575938  1.03334811 -0.03990541]\n",
      "Error: 0.0033313054558089675\n",
      "\n",
      "Weight: [ 0.03316312  1.03125323 -0.03719415]\n",
      "Error: 0.0028919416227737734\n",
      "\n",
      "Weight: [ 0.03077659  1.02926689 -0.03466732]\n",
      "Error: 0.002511053608117256\n",
      "\n",
      "Weight: [ 0.02857892  1.02738842 -0.03231233]\n",
      "Error: 0.0021806703520253884\n",
      "\n",
      "Weight: [ 0.02655193  1.02561587 -0.03011746]\n",
      "Error: 0.0018939739123713475\n",
      "\n",
      "Weight: [ 0.02467977  1.02394634 -0.02807179]\n",
      "Error: 0.0016451096996342332\n",
      "\n",
      "Weight: [ 0.02294852  1.0223763  -0.02616516]\n",
      "Error: 0.0014290353984827077\n",
      "\n",
      "Weight: [ 0.02134585  1.02090174 -0.0243881 ]\n",
      "Error: 0.0012413985592149145\n",
      "\n",
      "Weight: [ 0.01986085  1.01951839 -0.02273179]\n",
      "Error: 0.0010784359268087556\n",
      "\n",
      "Weight: [ 0.01848376  1.01822183 -0.02118801]\n",
      "Error: 0.0009368896209360312\n",
      "\n",
      "Weight: [ 0.01720586  1.0170076  -0.01974911]\n",
      "Error: 0.0008139366504753339\n",
      "\n",
      "Weight: [ 0.01601927  1.01587123 -0.01840796]\n",
      "Error: 0.0007071291752624441\n",
      "\n",
      "Weight: [ 0.01491689  1.01480835 -0.01715791]\n",
      "Error: 0.0006143435674831474\n",
      "\n",
      "Weight: [ 0.01389228  1.0138147  -0.01599277]\n",
      "Error: 0.00053373677328488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Street Light: Teach a neural network to translate a streetlight pattern into the correct stop/walk pattern.\n",
    "\n",
    "1. The important takeaway is that an infinite number of matrices exist that perfectly reflect the streetlight patterns in the dataset. It’s\n",
    "   important to recognize that the underlying pattern isn’t the same as the matrix. It’s a property of the matrix. In fact, it’s a\n",
    "   property of all of these matrices. The pattern is what each of these matrices is expressing. The pattern also existed in the streetlights.\n",
    "\n",
    "2. This input data pattern is what you want the neural network to learn to transform into the output data pattern.\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(streetlights, walk_vs_stop, weights, alpha):\n",
    "    for iteration in range(40):\n",
    "        error_for_all_lights = 0\n",
    "        for row_index in range(len(walk_vs_stop)):\n",
    "            input = streetlights[row_index]\n",
    "            goal_prediction = walk_vs_stop[row_index]\n",
    "        \n",
    "            prediction = input.dot(weights)\n",
    "            error = (goal_prediction - prediction) ** 2\n",
    "            error_for_all_lights += error\n",
    "\n",
    "            delta = prediction - goal_prediction\n",
    "            weights = weights - (alpha * (input * delta))\n",
    "        print(\"Weight:\", weights)\n",
    "        print(\"Error: \" + str(error_for_all_lights) + \"\\n\")\n",
    "\n",
    "    \n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "# You want the neural network to take any matrix containing the same underlying pattern as streetlights and transform it\n",
    "# into a matrix that contains the underlying pattern of walk_vs_stop. \n",
    "streetlights = np.array([[ 1, 0, 1 ],\n",
    "                         [ 0, 1, 1 ],\n",
    "                         [ 0, 0, 1 ],\n",
    "                         [ 1, 1, 1 ],\n",
    "                         [ 0, 1, 1 ],\n",
    "                         [ 1, 0, 1 ]])\n",
    "walk_vs_stop = np.array([ 0, 1, 0, 1, 1, 0 ])\n",
    "\n",
    "neural_network(streetlights, walk_vs_stop, weights, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full/Batch/Stochastic Gradient Descent\n",
    "\n",
    "1. Ths idea of learning one example at a time is a variant on gradient descent called stochastic gradient descent, and it’s one of the\n",
    "   handful of methods that can be used to learn an entire dataset. It performs a prediction and weight update for each training example\n",
    "   separately. In other words, it takes the first streetlight, tries to predict it, calculates the weight_delta, and updates the weights.\n",
    "   Then it moves on to the second streetlight, and so on. It iterates through the entire dataset many times until it can find a weight\n",
    "   configuration that works well for all the training examples.\n",
    "\n",
    "2. Another method for learning an entire dataset is gradient descent (or average/full gradient descent). Instead of updating the weights once\n",
    "   for each training example, the network calculates the average weight_delta over the entire dataset, changing the weights only each time\n",
    "   it computes a full average.\n",
    "\n",
    "3. The third configuration that sort of splits the difference between stochastic gradient descent and full gradient descent. Instead of\n",
    "   updating the weights after just one example or after the entire dataset of examples, you choose a batch size (typically between 8 and 256)\n",
    "   of examples, after which the weights are updated.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t+\t+\t-\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.5   0.48 -0.7 ]\n",
      "New Weight: [ 0.540848  0.72112  -0.400432]\n",
      "Error: 2.6561231104\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.540848  0.72112  -0.400432]\n",
      "New Weight: [ 0.49944225  0.84194128 -0.28583891]\n",
      "Error: 0.9628701776715985\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.49944225  0.84194128 -0.28583891]\n",
      "New Weight: [ 0.43945363  0.91252555 -0.23598163]\n",
      "Error: 0.5509165866836797\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.43945363  0.91252555 -0.23598163]\n",
      "New Weight: [ 0.38100946  0.95888996 -0.20924067]\n",
      "Error: 0.36445836852222424\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.38100946  0.95888996 -0.20924067]\n",
      "New Weight: [ 0.32955689  0.99146612 -0.19117291]\n",
      "Error: 0.2516768662079895\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.32955689  0.99146612 -0.19117291]\n",
      "New Weight: [ 0.28576471  1.01500784 -0.17673288]\n",
      "Error: 0.17797575048089034\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.28576471  1.01500784 -0.17673288]\n",
      "New Weight: [ 0.24888702  1.03207349 -0.16412797]\n",
      "Error: 0.12864460733422164\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.24888702  1.03207349 -0.16412797]\n",
      "New Weight: [ 0.21787885  1.0442761  -0.15269426]\n",
      "Error: 0.09511036950476208\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.21787885  1.0442761  -0.15269426]\n",
      "New Weight: [ 0.19174429  1.05275286 -0.14216186]\n",
      "Error: 0.07194564247043436\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.19174429  1.05275286 -0.14216186]\n",
      "New Weight: [ 0.16962672  1.05835728 -0.1324006 ]\n",
      "Error: 0.05564914990717743\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.16962672  1.05835728 -0.1324006 ]\n",
      "New Weight: [ 0.15081598  1.0617496  -0.12333176]\n",
      "Error: 0.04394763937673939\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.15081598  1.0617496  -0.12333176]\n",
      "New Weight: [ 0.13473149  1.06344721 -0.11489715]\n",
      "Error: 0.035357967050948465\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.13473149  1.06344721 -0.11489715]\n",
      "New Weight: [ 0.12090095  1.06385744 -0.10704822]\n",
      "Error: 0.02890700056547436\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.12090095  1.06385744 -0.10704822]\n",
      "New Weight: [ 0.10894061  1.06330118 -0.099742  ]\n",
      "Error: 0.023951660591138853\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.10894061  1.06330118 -0.099742  ]\n",
      "New Weight: [ 0.0985384   1.06203086 -0.09293946]\n",
      "Error: 0.020063105176016144\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.0985384   1.06203086 -0.09293946]\n",
      "New Weight: [ 0.08944019  1.06024455 -0.08660484]\n",
      "Error: 0.016952094519447087\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "Old Weight: [ 0.08944019  1.06024455 -0.08660484]\n",
      "New Weight: [ 0.08143858  1.05809708 -0.08070513]\n",
      "Error: 0.014420818295271236\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t-\t0\t+\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.08143858  1.05809708 -0.08070513]\n",
      "New Weight: [ 0.07436386  1.05570893 -0.07520984]\n",
      "Error: 0.012331739998443648\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.07436386  1.05570893 -0.07520984]\n",
      "New Weight: [ 0.06807682  1.05317333 -0.07009074]\n",
      "Error: 0.010587393171639842\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.06807682  1.05317333 -0.07009074]\n",
      "New Weight: [ 0.06246285  1.05056186 -0.06532169]\n",
      "Error: 0.009117233405426495\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.06246285  1.05056186 -0.06532169]\n",
      "New Weight: [ 0.05742729  1.04792901 -0.06087842]\n",
      "Error: 0.00786904226904208\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.05742729  1.04792901 -0.06087842]\n",
      "New Weight: [ 0.05289162  1.04531576 -0.05673843]\n",
      "Error: 0.006803273214640502\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.05289162  1.04531576 -0.05673843]\n",
      "New Weight: [ 0.0487905   1.04275242 -0.0528808 ]\n",
      "Error: 0.005889303541837786\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.0487905   1.04275242 -0.0528808 ]\n",
      "New Weight: [ 0.04506924  1.04026098 -0.04928611]\n",
      "Error: 0.0051029252561172675\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.04506924  1.04026098 -0.04928611]\n",
      "New Weight: [ 0.04168187  1.03785688 -0.04593632]\n",
      "Error: 0.004424644608684828\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.04168187  1.03785688 -0.04593632]\n",
      "New Weight: [ 0.03858958  1.03555045 -0.04281463]\n",
      "Error: 0.0038385124412518303\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.03858958  1.03555045 -0.04281463]\n",
      "New Weight: [ 0.03575938  1.03334811 -0.03990541]\n",
      "Error: 0.0033313054558089675\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.03575938  1.03334811 -0.03990541]\n",
      "New Weight: [ 0.03316312  1.03125323 -0.03719415]\n",
      "Error: 0.0028919416227737734\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.03316312  1.03125323 -0.03719415]\n",
      "New Weight: [ 0.03077659  1.02926689 -0.03466732]\n",
      "Error: 0.002511053608117256\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.03077659  1.02926689 -0.03466732]\n",
      "New Weight: [ 0.02857892  1.02738842 -0.03231233]\n",
      "Error: 0.0021806703520253884\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.02857892  1.02738842 -0.03231233]\n",
      "New Weight: [ 0.02655193  1.02561587 -0.03011746]\n",
      "Error: 0.0018939739123713475\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.02655193  1.02561587 -0.03011746]\n",
      "New Weight: [ 0.02467977  1.02394634 -0.02807179]\n",
      "Error: 0.0016451096996342332\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.02467977  1.02394634 -0.02807179]\n",
      "New Weight: [ 0.02294852  1.0223763  -0.02616516]\n",
      "Error: 0.0014290353984827077\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.02294852  1.0223763  -0.02616516]\n",
      "New Weight: [ 0.02134585  1.02090174 -0.0243881 ]\n",
      "Error: 0.0012413985592149145\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.02134585  1.02090174 -0.0243881 ]\n",
      "New Weight: [ 0.01986085  1.01951839 -0.02273179]\n",
      "Error: 0.0010784359268087556\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.01986085  1.01951839 -0.02273179]\n",
      "New Weight: [ 0.01848376  1.01822183 -0.02118801]\n",
      "Error: 0.0009368896209360312\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.01848376  1.01822183 -0.02118801]\n",
      "New Weight: [ 0.01720586  1.0170076  -0.01974911]\n",
      "Error: 0.0008139366504753339\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.01720586  1.0170076  -0.01974911]\n",
      "New Weight: [ 0.01601927  1.01587123 -0.01840796]\n",
      "Error: 0.0007071291752624441\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.01601927  1.01587123 -0.01840796]\n",
      "New Weight: [ 0.01491689  1.01480835 -0.01715791]\n",
      "Error: 0.0006143435674831474\n",
      "\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "0\t0\t1\t-->\t0\t\t0\t0\t-\t-->\t0\n",
      "1\t1\t1\t-->\t1\t\t-\t-\t+\t-->\t1\n",
      "0\t1\t1\t-->\t1\t\t0\t+\t-\t-->\t1\n",
      "1\t0\t1\t-->\t0\t\t+\t0\t-\t-->\t0\n",
      "Old Weight: [ 0.01491689  1.01480835 -0.01715791]\n",
      "New Weight: [ 0.01389228  1.0138147  -0.01599277]\n",
      "Error: 0.00053373677328488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Neural networks learn correlation\n",
    "\n",
    "1. In the process of gradient descent, each training example asserts either up pressure or down pressure on the weights. On average,\n",
    "   there was more up pressure for the middle weight and more down pressure for the other weights.\n",
    "\n",
    "2. Each node is individually trying to correctly predict the output given the input. For the most part, each node ignores all the other\n",
    "   nodes when attempting to do so. The only cross communication occurs in that all three weights must share the same error measure. The\n",
    "   weight update is nothing more than taking this shared error measure and multiplying it by each respective input. A key part of why\n",
    "   neural networks learn is error attribution, which means given a shared error, the network needs to figure out which weights contributed\n",
    "   (so they can be adjusted) and which weights did not contribute (so they can be left alone). \n",
    "\n",
    "3. The Weight Pressure table helps describe the effect of each training example on each respective weight. + indicates that\n",
    "   it has pressure toward 1, and – indicates that it has pressure toward 0. Zeros (0) indicate that there is no pressure because\n",
    "   the input datapoint is 0, so that weight won’t be changed.\n",
    "\n",
    "4. The prediction is a weighted sum of the inputs. The learning algorithm rewards inputs that correlate with the output with upward pressure\n",
    "   (toward 1) on their weight while penalizing inputs with discorrelation with downward pressure. The weighted sum of the inputs find perfect\n",
    "   correlation between the input and the output by weighting decorrelated inputs to 0. (Rewarding correlation with pressure toward 1 and\n",
    "   penalizing decorrelation with pressure toward 0)\n",
    "\"\"\"\n",
    "\n",
    "def neural_network(streetlights, walk_vs_stop, weights, alpha):\n",
    "    for iteration in range(40):\n",
    "        error_for_all_lights = 0\n",
    "        old_weights = weights\n",
    "        for row_index in range(len(walk_vs_stop)):\n",
    "            input = streetlights[row_index]\n",
    "            goal_prediction = walk_vs_stop[row_index]\n",
    "        \n",
    "            prediction = input.dot(weights)\n",
    "            error = (goal_prediction - prediction) ** 2\n",
    "            error_for_all_lights += error\n",
    "\n",
    "            delta = prediction - goal_prediction\n",
    "            new_weights = weights - (alpha * (input * delta))\n",
    "            \n",
    "            pressue = []\n",
    "            data = []\n",
    "            for i in range(len(new_weights)):\n",
    "                if abs(new_weights[i]) < abs(weights[i]):\n",
    "                    pressue.append('-')\n",
    "                elif abs(new_weights[i]) > abs(weights[i]):\n",
    "                    pressue.append('+')\n",
    "                else:\n",
    "                    pressue.append('0')\n",
    "                data.append(str(streetlights[row_index].tolist()[i]))\n",
    "            data.append('-->\\t' + str(walk_vs_stop[row_index]))\n",
    "            pressue.append('-->\\t' + str(walk_vs_stop[row_index])) # append output\n",
    "            print(\"\\t\".join(data) + '\\t\\t' + \"\\t\".join(pressue))\n",
    "            weights = new_weights\n",
    "        \n",
    "        print(\"Old Weight:\", old_weights)\n",
    "        print(\"New Weight:\", new_weights)\n",
    "        print(\"Error: \" + str(error_for_all_lights) + \"\\n\")\n",
    "    \n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "neural_network(streetlights, walk_vs_stop, weights, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Edge Cases:\n",
    "\n",
    "1. Deep learning's greatest weakness: Overfitting. Error is shared among all the weights. If a particular configuration of weights\n",
    "   accidentally creates perfect correlation between the prediction and the output dataset (such that error == 0) without giving the heaviest\n",
    "   weight to the best inputs, the neural network will stop learning. （Sometimes correlation hanpens accidentally）\n",
    "\n",
    "2. Neural networks are so flexible that they can find many, many different weight configurations that will correctly predict for a subset of\n",
    "   training data. If you trained this neural network on the first two training examples, it would likely stop learning at a point where it\n",
    "   did not work well for the other training examples. In essence, it memorized the two training examples instead of finding the correlation\n",
    "   that will generalize to any possible streetlight configuration. \n",
    "\n",
    "3. The greatest challenge you’ll face with deep learning is convincing your neural network to generalize instead of just memorize.\n",
    "\n",
    "4. Edge case 2: Conflicting pressure. Sometimes correlation fights itself.\n",
    "   <Training Data>              <Weight Pressure>\n",
    "   1  0  1  -->  0\t\t\t\t-  0  -  -->  0\n",
    "   0  1  1  -->  1\t\t\t\t0  +  +  -->  1\n",
    "   0  0  1  -->  0\t\t\t\t0  0  -  -->  0\n",
    "   1  1  1  -->  1\t\t\t\t+  +  +  -->  1\n",
    "   0  1  1  -->  1\t\t\t\t0  +  +  -->  1\n",
    "   1  0  1  -->  0\t\t\t\t-  0  -  -->  0\n",
    "   This column seems to have an equal number of upward and downward pressure moments. But the network correctly pushes this (far-right)\n",
    "   weight down to 0, which means the downward pressure moments must be larger than the upward ones. How does this work? As other nodes learn,\n",
    "   they absorb some of the error; they absorb part of the correlation. They cause the network to predict with moderate correlative power,\n",
    "   which reduces the error. The other weights then only try to adjust their weights to correctly predict what’s left. In this case, because\n",
    "   the middle weight has consistent signal to absorb all the correlation (because of the 1:1 relationship between the middle input and the\n",
    "   output), the error when you want to predict 1 becomes very small, but the error to predict 0 becomes large, pushing the middle weight\n",
    "   downward. \n",
    "\n",
    "5. As a preview, the regularization is advantageous because if a weight has equal pressure upward and downward, it isn’t good for anything.\n",
    "   It’s not helping either direction. In essence, regularization aims to say that only weights with really strong correlation can stay on;\n",
    "   everything else should be silenced because it’s contributing noise. It’s sort of like natural selection, and as a side effect it would\n",
    "   cause the neural network to train faster.\n",
    "\n",
    "6. There is no correlation between any input column and the output column. Every weight has an equal amount of upward pressure and downward\n",
    "   pressure. This dataset is a real problem for the neural network. Previously, you could solve for input datapoints that had both upward\n",
    "   and downward pressure because other nodes would start solving for either the positive or negative predictions, drawing the balanced node\n",
    "   to favor up or down. But in this case, all the inputs are equally balanced between positive and negative pressure.\n",
    "   <Training Data>              <Weight Pressure>\n",
    "   1  0  1  -->  1\t\t\t\t+  0  +  -->  1\n",
    "   0  1  1  -->  1\t\t\t\t0  +  +  -->  1\n",
    "   0  0  1  -->  0\t\t\t\t0  0  -  -->  0\n",
    "   1  1  1  -->  0\t\t\t\t-  -  -  -->  0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Learning Indirect Correlation\n",
    "\n",
    "1. Previously, I described a neural network as an instrument that searches for correlation between input and output datasets. I want to\n",
    "   refine this just a touch. In reality, neural networks search for correlation between their input and output layers.\n",
    "\n",
    "2. Because the input dataset doesn’t correlate with the output dataset, you’ll use the input dataset to create an intermediate dataset\n",
    "   that does have correlation with the output. You basically stack two neural networks on top of each other. The middle layer of nodes\n",
    "   (layer_1) represents the intermediate dataset. The goal is to train this network so that even though there’s no correlation between\n",
    "   the input dataset and output dataset (layer_0 and layer_2), the layer_1 dataset that you create using layer_0 will have correlation\n",
    "   with layer_2.\n",
    "\n",
    "3. The output of the first lower network (layer_0 to layer_1) is the input to the second upper neural network (layer_1 to layer_2). The\n",
    "   prediction for each of these networks is identical to what you saw before. If you ignore the lower weights and consider their output\n",
    "   to be the training set, the top half of the neural network (layer_1 to layer_2) is just like the networks trained in the preceding\n",
    "   chapter. You can use all the same learning logic to help them learn. The part that you don’t yet understand is how to update the weights\n",
    "   between layer_0 and layer_1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backpropagation:  Long-distance error attribution\n",
    "\n",
    "1. What’s the prediction from layer_1 to layer_2? It’s a weighted average of the values at layer_1. If layer_2 is too high by x amount,\n",
    "   how do you know which values at layer_1 contributed to the error? The ones with higher weights contributed more. The ones with lower\n",
    "   weights from layer_1 to layer_2 contributed less. \n",
    "\n",
    "2. Important Notes: The weights from layer_1 to layer_2 exactly describe how much each layer_1 node contributes to the layer_2 prediction.\n",
    "   This means those weights also exactly describe how much each layer_1 node contributes to the layer_2 error. \n",
    "\n",
    "3. How do you use the delta at layer_2 to figure out the delta at layer_1? You multiply it by each of the respective weights for layer_1.\n",
    "   It’s like the prediction logic in reverse. This process of moving delta signal around is called backpropagation.\n",
    "\n",
    "4. The delta variable told you the direction and amount the value of this node should change next time. If you want this node to be x amount\n",
    "   higher, then each of these previous four nodes needs to be x*weights_1_2 amount higher/lower, because these weights were amplifying the\n",
    "   prediction by weights_1_2 times. When used in reverse, the weights_1_2 matrix amplifies the error by the appropriate amount. It amplifies\n",
    "   the error so you know how much each layer_1 node should move up or down. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Linear vs Nonlinear\n",
    "\n",
    "1. For any two consecutive weighted sums of the input, there exists a single weighted sum with exactly identical behavior. Anything that the\n",
    "   three-layer network can do, the two-layer network can also do. Stacking two neural nets doesn’t give you any more power. Two consecutive\n",
    "   weighted sums is just a more expensive version of one weighted sum.\n",
    "\n",
    "2. Each node in the middle layer subscribes to a certain amount of correlation with each input node. If the weight from an input to the\n",
    "   middle layer is 1.0, then it subscribes to exactly 100% of that node’s movement. If that node goes up by 0.3, the middle node will\n",
    "   follow. If the weight connecting two nodes is 0.5, each node in the middle layer subscribes to exactly 50% of that node’s movement. \n",
    "   The middle nodes don’t get to have correlation of their own. They’re more or less correlated to various input nodes. But because\n",
    "   you know that in the new dataset there is no correlation between any of the inputs and the output, how can the middle layer help?\n",
    "   It mixes up a bunch of correlation that’s already useless. What you really need is for the middle layer to be able to selectively\n",
    "   correlate with the input. \n",
    "\n",
    "3. You want the middle layer to sometimes correlate with an input, and sometimes not correlate. That gives it correlation of its own.\n",
    "   This gives the middle layer the opportunity to not just always be x% correlated to one input and y% correlated to another input. Instead,\n",
    "   it can be x% correlated to one input only when it wants to be, but other times not be correlated at all. This is called conditional\n",
    "   correlation or sometimes correlation.\n",
    "\n",
    "4. By turning off any middle node whenever it would be negative, you allow the network to sometimes subscribe to correlation from various\n",
    "   inputs. This is impossible for two-layer neural networks, thus adding power to three-layer nets.\n",
    "\n",
    "5. The fancy term for this “if the node would be negative, set it to 0” logic is nonlinearity. Without this tweak, the neural network is\n",
    "   linear. Without this technique, the output layer only gets to pick from the same correlation it had in the two-layer network.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.6342311598444467\n",
      "Error:0.35838407676317513\n",
      "Error:0.0830183113303298\n",
      "Error:0.006467054957103705\n",
      "Error:0.0003292669000750734\n",
      "Error:1.5055622665134859e-05\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The First Deep Neural Network\n",
    "\n",
    "1. Previously, you learned: You can compute the relationship between the error and any one of the weights so that you know how changing\n",
    "   the weight changes the error. You can then use this to reduce the error to 0.\n",
    "\n",
    "2. Now: Adjusting the weights to reduce the error over a series of training examples ultimately searches for correlation between the input\n",
    "   and the output layers. If no correlation exists, then the error will never reach 0.\n",
    "   \n",
    "3. How do you calculate the deltas for layer_1? First, do the obvious: multiply the output delta by each weight attached to it. This gives\n",
    "   a weighting of how much each weight contributed to that error. There’s one more thing to factor in. If relu set the output to a\n",
    "   layer_1 node to be 0, then it didn’t contribute to the error. When this is true, you should also set the delta of that node to 0.\n",
    "   Multiplying each layer_1 node by the relu2deriv function accomplishes this. relu2deriv is either 1 or 0, depending on whether the layer_1\n",
    "   value is greater than 0.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "# nonlinearity\n",
    "def relu(x):\n",
    "    # returns x if x > 0 otherwise return 0\n",
    "    return (x > 0) * x \n",
    "\n",
    "def relu2deriv(output):\n",
    "    # returns 1 if output > 0 otherwise return 0\n",
    "    return output > 0\n",
    "\n",
    "# hyper parameters\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "# input & ouput\n",
    "streetlights = np.array([[ 1, 0, 1 ],\n",
    "                         [ 0, 1, 1 ],\n",
    "                         [ 0, 0, 1 ],\n",
    "                         [ 1, 1, 1 ]])\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
    "\n",
    "# weight initialization\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1  # make weights belong to the range of (-1, 1)\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "# predict, compare and learn\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "        \n",
    "        # squared sum error\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "\n",
    "        # delta in layer2 and layer1\n",
    "        layer_2_delta = (walk_vs_stop[i:i+1] - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # only new code\n",
    "\n",
    "        # update weight\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "    if(iteration % 10 == 9):\n",
    "        print(\"Error:\" + str(layer_2_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1.47\t1.47\t0\t0\t0\t0\t0\t\n",
      "\n",
      "1.47\t0\t0\t1.87\t0\t1.87\t0\t0\t\n",
      "\n",
      "1.47\t0\t0\t0\t0\t0\t2.56\t0\t\n",
      "\n",
      "0\t1.87\t0\t0\t2.56\t0\t0\t0\t\n",
      "\n",
      "0\t0\t0\t2.56\t0\t0\t0\t1.47\t\n",
      "\n",
      "0\t1.87\t0\t0\t0\t0\t0\t1.47\t\n",
      "\n",
      "0\t0\t2.56\t0\t0\t0\t0\t1.47\t\n",
      "\n",
      "0\t0\t0\t0\t1.47\t1.47\t1.47\t0\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "l1 = [0, 2, 1, 0, 0, 0, 0, 0]\n",
    "l2 = [2, 0, 0, 1, 0, 1, 0, 0]\n",
    "l3 = [1, 0, 0, 0, 0, 0, 1, 0]\n",
    "l4 = [0, 1, 0, 0, 1, 0, 0, 0]\n",
    "l5 = [0, 0, 0, 1, 0, 0, 0, 1]\n",
    "l6 = [0, 1, 0, 0, 0, 0, 0, 1]\n",
    "l7 = [0, 0, 1, 0, 0, 0, 0, 1]\n",
    "l8 = [0, 0, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "ll = [3, 2, 1, 1, 1, 1, 1, 3] \n",
    "t  = 13\n",
    "\n",
    "matrix = []\n",
    "matrix.append(l1)\n",
    "matrix.append(l2)\n",
    "matrix.append(l3)\n",
    "matrix.append(l4)\n",
    "matrix.append(l5)\n",
    "matrix.append(l6)\n",
    "matrix.append(l7)\n",
    "matrix.append(l8)\n",
    "\n",
    "for x in range(len(l1)):\n",
    "    for y in range(len(l2)):\n",
    "        if matrix[x][y] != 0:\n",
    "            value = float(matrix[x][y] * t) / float(ll[x] * ll[y])\n",
    "            matrix[x][y] = math.log(value)\n",
    "\n",
    "for x in range(len(l1)):\n",
    "    for y in range(len(l2)):\n",
    "        if matrix[x][y] > 0:\n",
    "            print('%.2f' % matrix[x][y], end='\\t')\n",
    "        else:\n",
    "            print(0, end='\\t')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
